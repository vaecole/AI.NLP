{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷没有标签。\n",
      " 卷的序列号是 D4A3-95FD\n",
      "\n",
      " C:\\Users\\Mark\\1AI.NLP\\AI.NLP\\data 的目录\n",
      "\n",
      "2019-02-09  21:46    <DIR>          .\n",
      "2019-02-09  21:46    <DIR>          ..\n",
      "2017-12-03  12:09       160,966,723 sqlResult_1558435.csv\n",
      "               1 个文件    160,966,723 字节\n",
      "               2 个目录 21,873,483,776 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv('../data/sqlResult_1558435.csv', header=0, encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'source', 'content', 'feature', 'title', 'url'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考虑性能而去屏蔽掉小核心。相反，他们正联手微软，找到一种适合桌面平台的、兼顾性能和功耗的完美方案。报道称，微软已经拿到了一些新的源码，以便Windows 10更好地理解big.little架构。资料显示，骁龙835作为一款集成了CPU、GPU、基带、蓝牙/Wi-Fi的SoC，比传统的Wintel方案可以节省至少30%的PCB空间。按计划，今年Q4，华硕、惠普、联想将首发骁龙835 Win10电脑，预计均是二合一形态的产品。当然，高通骁龙只是个开始，未来也许还能见到三星Exynos、联发科、华为麒麟、小米澎湃等进入Windows 10桌面平台。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv['content'][1].replace('\\r\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "obj = json.loads(data_csv['feature'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'网易热门'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj['site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_corpus_source(n):\n",
    "    corpus,sources=[],[]\n",
    "    for i in range(n):\n",
    "        if isinstance(data_csv['content'][i], float): # 排除float型空行\n",
    "            corpus.append('')\n",
    "            sources.append('x')\n",
    "            continue\n",
    "            \n",
    "        corpus.append(cut(data_csv['content'][i].replace('\\r\\n','').replace(u'\\u3000','')))\n",
    "        if 'site' in data_csv['feature'][i]: sources.append(json.loads(data_csv['feature'][i])['site'])\n",
    "        else: sources.append(\"x\")\n",
    "    return (corpus, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Mark\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.837 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "c_s = get_top_corpus_source(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = c_s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = vectorizer.fit_transform(c_s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ary = tfidf_vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9102)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9102, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ary.transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(train_rate * len(c_s[0]))\n",
    "train_X = tfidf_ary[:train_index]\n",
    "train_Y = c_s[1][:train_index]\n",
    "\n",
    "test_X = tfidf_ary[train_index:]\n",
    "test_Y = c_s[1][train_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cnbeta', '环球', '环球', 'cnbeta', '环球', 'cnbeta', 'cnbeta', 'cnbeta',\n",
       "       '环球', 'cnbeta', '环球', 'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta', '环球',\n",
       "       'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta',\n",
       "       'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta', 'cnbeta',\n",
       "       'cnbeta', 'cnbeta'], dtype='<U6')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.06353837],\n",
       "       [0.        , 0.23259552, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def cut(content):\n",
    "    return ' '.join(jieba.cut(content))\n",
    "\n",
    "\n",
    "def get_top_corpus_source(n, filename='../data/sqlResult_1558435.csv', encoding='gb18030'):\n",
    "    data_csv = pd.read_csv(filename, header=0, encoding=encoding)\n",
    "    corpus, sources = [], []\n",
    "    for i in range(n):\n",
    "        if isinstance(data_csv['content'][i], float):  # 排除float型空行\n",
    "            corpus.append('')\n",
    "            sources.append('x')\n",
    "            continue\n",
    "\n",
    "        corpus.append(cut(data_csv['content'][i].replace('\\r\\n', '').replace(u'\\u3000', '')))\n",
    "        if 'site' in data_csv['feature'][i]:\n",
    "            sources.append(json.loads(data_csv['feature'][i])['site'])\n",
    "        else:\n",
    "            sources.append(\"x\")\n",
    "    return corpus, sources\n",
    "\n",
    "\n",
    "def tf_idf(data_set):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector = vectorizer.fit_transform(data_set[0])\n",
    "    return vector.toarray(), data_set[1]\n",
    "\n",
    "\n",
    "def split_data_set(data_set, training_rate=0.7):\n",
    "    train_index = int(training_rate * len(data_set[1]))\n",
    "    train_x = data_set[0][:train_index]\n",
    "    train_y = data_set[1][:train_index]\n",
    "\n",
    "    test_x = data_set[0][train_index:]\n",
    "    test_y = data_set[1][train_index:]\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "def train_by_lr(train_x, train_y):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_x, train_y)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(method_func, train_x, train_y):\n",
    "    method_func.fit(train_x, train_y)\n",
    "    return method_func\n",
    "\n",
    "\n",
    "def predict(method_func, test_x):\n",
    "    return method_func.predict(test_x)\n",
    "\n",
    "\n",
    "corpus_source = get_top_corpus_source(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf_idf(corpus_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_data = split_data_set(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnbeta' '环球' '环球' 'cnbeta' '环球' 'cnbeta' 'cnbeta' 'cnbeta' '环球' 'cnbeta'\n",
      " '环球' 'cnbeta' 'cnbeta' 'cnbeta' 'cnbeta' '环球' 'cnbeta' 'cnbeta' 'cnbeta'\n",
      " 'cnbeta' 'cnbeta' 'cnbeta' 'cnbeta' 'cnbeta' 'cnbeta' 'cnbeta' 'cnbeta'\n",
      " 'cnbeta' 'cnbeta' 'cnbeta']\n",
      "['cnbeta', '环球', '环球', 'cnbeta', '环球', 'cnbeta', 'cnbeta', '凤凰', '环球', 'cnbeta', '环球', '新浪', 'cnbeta', '凤凰', '参考消息', '环球', '参考消息', 'cnbeta', '参考消息', '参考消息', '环球', 'cnbeta', '参考消息', '凤凰', '参考消息', '环球', '参考消息', 'cnbeta', '参考消息', '参考消息']\n"
     ]
    }
   ],
   "source": [
    "lr_recognizer = train_by_lr(tt_data[0],tt_data[1])\n",
    "predict_rs = predict(lr_recognizer, tt_data[2])\n",
    "print(str(predict_rs))\n",
    "print(tt_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(t,p):\n",
    "    tpc = 0\n",
    "    for i in range(len(t)):\n",
    "        if t[i] == p[i]:\n",
    "            tpc = tpc + 1\n",
    "    return float(tpc/len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(tt_data[3], predict_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

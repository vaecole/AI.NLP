{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Web Crawler\n",
    "- 给出一个种子URL\n",
    "- 对于一个URl对应的网页，获取该网页上的所有链接（a标签）列表，递归获取所有的URL（可以使用Redis存储和判重）\n",
    "- 下载有效网页的内容到本地\n",
    "- 对于给定的网页内容，爬取有效文本内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Redis作为一个缓存服务，用来记录已经下载的网页（默认有效期90天），还可用来判断某个网页是否被下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis    # 导入redis模块，通过python操作redis 也可以直接在redis主机的服务端操作缓存数据库\n",
    "\n",
    "\n",
    "redis_pool = redis.ConnectionPool(host='127.0.0.1', port=6379, decode_responses=True) \n",
    "redis_url_visit_pre = 'url_visited:'\n",
    "\n",
    "def append_visited(url, filename):\n",
    "    if not filename:\n",
    "        return False\n",
    "    \n",
    "    redis_conn = redis.Redis(connection_pool = redis_pool)\n",
    "    key = redis_url_visit_pre + url\n",
    "    if not redis_conn.exists(key):\n",
    "        redis_conn.set(key, filename, ex = 90 * 24 * 60 * 60)\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_visited(url):\n",
    "    redis_conn = redis.Redis(connection_pool = redis_pool)\n",
    "    return redis_conn.exists(redis_url_visit_pre + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 此方法访问一个URL，如果成功则获取对应网页的所有有效href，同时保存网页到本地并且更新缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import pathname2url\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "\n",
    "def build_logger():\n",
    "    log_base = 'log'\n",
    "    if not os.path.exists(log_base):\n",
    "        os.mkdir(log_base)\n",
    "\n",
    "    if not os.path.exists(os.path.join(log_base,'error')):\n",
    "        os.mkdir(os.path.join(log_base,'error'))\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s',\n",
    "                        filename='log/error/crawler0104.log', \n",
    "                        level = logging.ERROR,\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(log_base,'warning')):\n",
    "        os.mkdir(os.path.join(log_base,'warning'))\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s',\n",
    "                        filename='log/warning/crawler0104.log', \n",
    "                        level = logging.WARNING,\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "build_logger()\n",
    "\n",
    "\n",
    "def visit(url, limit_hostname = ''):\n",
    "    url_set = set()\n",
    "    \n",
    "    if is_visited(url):\n",
    "        return url_set\n",
    "    \n",
    "    try:\n",
    "        page = requests.get(url) \n",
    "    except requests.ConnectionError:\n",
    "        print(\"Error: ConnectionError when request \"+ url)\n",
    "        return url_set\n",
    "    except Exception as e:\n",
    "        logging.warning('request error for ' + url + ', see error info.')\n",
    "        logging.error(str(e))\n",
    "        return url_set\n",
    "    \n",
    "    if not page.ok:\n",
    "        print(url + ' return ' + str(page.status_code))\n",
    "        return url_set\n",
    "    \n",
    "    parsed_uri = urlparse(url)\n",
    "    folder = urllib.parse.quote(parsed_uri.hostname, safe='')\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    filename = os.path.join(folder, urllib.parse.quote(url, safe=''))\n",
    "    \n",
    "    page_html = page.text\n",
    "    if not is_visited(url):\n",
    "        page_encode = page.encoding\n",
    "        try:\n",
    "            with open(filename,'w',encoding = page_encode) as file_write:\n",
    "                file_write.write(page_html)\n",
    "        except Exception as e:\n",
    "            logging.warning('writing file error for ' + url + ', see error info.')\n",
    "            logging.error(str(e))\n",
    "    \n",
    "    append_visited(url, filename)\n",
    "    \n",
    "    soup = BeautifulSoup(page_html)\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get(\"href\")\n",
    "        if href and href != '#' and href != './' and href != 'javascript:;' and 'http' in href and '.html' in href and not is_visited(href):\n",
    "            if not limit_hostname or limit_hostname in href:\n",
    "                url_set.add(href)\n",
    "            \n",
    "    return url_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo建立网页连接图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def recurrent_visit(seed_url, limit_hostname = '', limit_pages = 1000*1000):\n",
    "    all_urls = set([seed_url])\n",
    "    count = 0\n",
    "    while all_urls:\n",
    "        print(len(all_urls))\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        url = all_urls.pop()\n",
    "        next_urls = visit(url, limit_hostname)\n",
    "        if next_urls and count < limit_pages:\n",
    "            all_urls.update(next_urls)\n",
    "            display(next_urls)\n",
    "        count += 1\n",
    "        display(count)\n",
    "    print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://www.cmiw.cn/thread-307172-2-1.html',\n",
       " 'http://www.cmiw.cn/thread-307172-4-1.html'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85327\n"
     ]
    }
   ],
   "source": [
    "recurrent_visit(\"http://www.cmiw.cn/\",\"cmiw\",5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

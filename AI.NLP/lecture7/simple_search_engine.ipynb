{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本实验利用爬虫爬到的网页页面，利用TF-IDF和PageRank分别构成两种搜索引擎\n",
    "- 分析TF-IDF和搜索关键字，找到网页\n",
    "- 分别计算TD-IDF和PageRank来给网页排序\n",
    "\n",
    "### 文件夹www.cmiw.cn 中的都是爬虫web_crawler.ipynb爬下来的html文件，并且保留了网页链接关系文件linked_url.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷没有标签。\n",
      " 卷的序列号是 D4A3-95FD\n",
      "\n",
      " C:\\Users\\Mark\\1AI.NLP\\AI.NLP\\lecture7 的目录\n",
      "\n",
      "2019-01-11  14:50    <DIR>          .\n",
      "2019-01-11  14:50    <DIR>          ..\n",
      "2019-01-04  16:34    <DIR>          .ipynb_checkpoints\n",
      "2019-01-11  14:49    <DIR>          log\n",
      "2019-01-11  14:48            48,643 simple_search_engine.ipynb\n",
      "2019-01-11  14:50            12,206 web_crawler.ipynb\n",
      "2019-01-11  14:51    <DIR>          www.cmiw.cn\n",
      "2019-01-04  19:37    <DIR>          www.cmiw.cn_text_\n",
      "               2 个文件         60,849 字节\n",
      "               6 个目录 26,884,120,576 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_base = \"www.cmiw.cn\" # html folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫爬到的是HTML文件，文本中含有标签，这里使用html2text将html处理为文本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import html2text\n",
    "\n",
    "html_handler = html2text.HTML2Text()\n",
    "html_handler.ignore_links = True\n",
    "\n",
    "def extract_text_from_html(html_folder):\n",
    "    text_folder = html_folder + '_text'\n",
    "    if not os.path.isdir(text_folder):\n",
    "        os.mkdir(text_folder)\n",
    "    html_files = [html_f for html_f in os.listdir(html_folder) if '.html' in html_f and ('article' in html_f or 'thread' in html_f)]\n",
    "    for f in tqdm(html_files):\n",
    "        if not os.path.exists(os.path.join(text_folder, f)):\n",
    "            with open(os.path.join(html_folder, f)) as html_file:\n",
    "                with open(os.path.join(text_folder, f),'w',encoding='utf-8') as txt_file:\n",
    "                    txt_file.write(html_handler.handle(html_file.read()))\n",
    "    \n",
    "    return text_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 663/663 [00:37<00:00, 17.66it/s]\n"
     ]
    }
   ],
   "source": [
    "text_base = extract_text_from_html(html_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "len(os.listdir(text_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有的文本文件先各自分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def cut(string): return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def cut_text(text_folder):\n",
    "    cut_folder = text_folder + '_cut'\n",
    "    if not os.path.isdir(cut_folder):\n",
    "        os.mkdir(cut_folder)\n",
    "        \n",
    "    for f in tqdm(os.listdir(text_folder)):\n",
    "        if not os.path.exists(os.path.join(cut_folder, f)):\n",
    "            with open(os.path.join(text_folder, f), encoding = 'utf-8') as text_file:\n",
    "                with open(os.path.join(cut_folder, f), 'w',encoding = 'utf-8') as cut_file:\n",
    "                    cut_file.write(cut(text_file.read()))\n",
    "    \n",
    "    return cut_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 663/663 [00:16<00:00, 39.81it/s]\n"
     ]
    }
   ],
   "source": [
    "cut_base = cut_text(text_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载网页路径名称和网页语料库\n",
    "- 路径相当与网址，搜索结果给出路径即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_webpage_corpus(folder):\n",
    "    files = os.listdir(folder)\n",
    "    corpus = []\n",
    "    for i in tqdm(range(len(files))):\n",
    "        with open(os.path.join(folder, files[i]), encoding = 'utf-8') as cut_file:\n",
    "            corpus += [cut_file.read()]\n",
    "    return (files, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 663/663 [00:00<00:00, 2483.13it/s]\n"
     ]
    }
   ],
   "source": [
    "webpage_corpus = load_webpage_corpus(cut_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer 可以将文章进行TF-IDF向量化\n",
    "- TF-IDF值表示词在某篇文章的重要性\n",
    "- 本实验共有663篇文章，39223个词汇\n",
    "- 如果文章没有该词，则TF-IDF为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(webpage_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_vec = vectorizer.fit_transform(corpus)\n",
    "    print(tf_idf_vec.shape)\n",
    "    return (vectorizer.vocabulary_, tf_idf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(663, 39223)\n"
     ]
    }
   ],
   "source": [
    "voca_vec = vectorize(webpage_corpus[1])\n",
    "voca = voca_vec[0]\n",
    "doc_word_vec = voca_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_array = doc_word_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转置之后就是词的倒排索引\n",
    "- 若第v个词在第r篇文章中则word_doc_vec[v][r]不为0\n",
    "- word_doc_vec[v][r]这个值表示了词在文章中的重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39223, 663)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_doc_vec = doc_word_vec.transpose()\n",
    "word_doc_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_array = word_doc_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentence, vocabulary):\n",
    "    return [vocabulary.get(c) for c in cut(sentence).split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据词的编号找到索引的所有文章TF-IDF数组，然后用np.where(array)可以得到数组非0的坐标（数组索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import and_\n",
    "from functools import reduce\n",
    "\n",
    "def find_docs_by_sentence(sentence, vocabulary, word_doc_vector):\n",
    "    if not sentence:\n",
    "        return None\n",
    "    \n",
    "    word_ids = [vocabulary.get(c) for c in cut(sentence).split()]\n",
    "    found_doc_indexes = []\n",
    "    for word_id in word_ids:\n",
    "        found_doc_indexes.append(set(np.where(word_doc_vector[word_id])[0]))\n",
    "    \n",
    "    if not found_doc_indexes:\n",
    "        return None\n",
    "    \n",
    "    common_doc_ids = reduce(and_, found_doc_indexes)\n",
    "    return common_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{123, 635}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_docs_by_sentence(\"深圳机械师傅\", voca, word_doc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cmiw.cn/thread-240107-1-1.html\n",
      "http://www.cmiw.cn/thread-967758-1-1.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "\n",
    "# 经过检查可以在一下网页找到 深圳 机械 师傅\n",
    "print(urllib.parse.unquote(webpage_corpus[0][123])) \n",
    "print(urllib.parse.unquote(webpage_corpus[0][635])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里使用关键词在文章中的TF-IDF值的和来确定搜索结果排序\n",
    "- 搜索关键词有w1,w2\n",
    "- 包含关键词w1,w2的文章有d1,d2，都是搜索结果\n",
    "- 求w1,w2在搜索结果的文章中的TF-IDF值的和sum1, sum2，越大的代表越重要\n",
    "- 根据重要程度，实验中，文章635应该在文章123之前，因为前者的sum_Tfidf更大\n",
    "- 实验没有使用课堂上的Cosine距离，因为从原理上说关键词的向量应该与文章的向量的关系应该不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def sum_Tfidf(sentence, Tfidf_doc, vocabulary = voca):\n",
    "    cut_word_ids = get_word_ids(sentence, vocabulary)\n",
    "    return sum([Tfidf_doc[word_id] for word_id in cut_word_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015197336243449423\n",
      "0.11507366592701099\n"
     ]
    }
   ],
   "source": [
    "print(sum_Tfidf(\"深圳机械师傅\", doc_word_array[123]))\n",
    "print(sum_Tfidf(\"深圳机械师傅\", doc_word_array[635]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用词的TF-IDF加权值来对文章进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def search_webpage_STI(keywords, vocabulary = voca, doc_word_ary = doc_word_array, sort_strategy):\n",
    "    if not keywords:\n",
    "        return None\n",
    "    \n",
    "    found_doc_ids = find_docs_by_sentence(keywords, vocabulary, doc_word_ary.transpose())\n",
    "    if not found_doc_ids:\n",
    "        return None\n",
    "    \n",
    "    sum_TI_sorted_ids = sorted(found_doc_ids, key = lambda x: sum_Tfidf(keywords, doc_word_ary[x], vocabulary), reverse = True)\n",
    "\n",
    "    return sum_TI_sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = search_webpage_STI(\"深圳机械师傅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os        \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_webpage(doc_index, webpages):\n",
    "    print(urllib.parse.unquote(webpages[doc_index]))\n",
    "    return webpages[doc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cmiw.cn/thread-967758-1-1.html\n",
      "http://www.cmiw.cn/thread-240107-1-1.html\n"
     ]
    }
   ],
   "source": [
    "url_results = [show_webpage(id_, webpage_corpus[0]) for id_ in sorted_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank\n",
    "### Refference https://blog.csdn.net/hguisu/article/details/7996185"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对网页关系的图进行PageRank计算\n",
    "- linked_url.json是在爬虫爬网页时建立的网页关系数据结构（dict）\n",
    "- 与课上不一样，PageRank应该使用单向图，所以使用nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_linked_urls(filename):\n",
    "    with open(filename, encoding= 'utf-8') as json_file:\n",
    "        return json.loads(json_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_urls = get_linked_urls(\"linked_url.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "webpage_network = nx.DiGraph(linked_urls)\n",
    "ranked_webpages = nx.pagerank(webpage_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def search_webpage_with_sort(keywords, sort_strategy, vocabulary = voca, doc_word_ary = doc_word_array):\n",
    "    if not keywords:\n",
    "        return None\n",
    "    \n",
    "    found_doc_ids = find_docs_by_sentence(keywords, vocabulary, doc_word_ary.transpose())\n",
    "    if not found_doc_ids:\n",
    "        return None\n",
    "\n",
    "    sum_TI_sorted_ids = sorted(found_doc_ids, key = lambda x: sort_strategy(x), reverse = True)\n",
    "    return sum_TI_sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_rank_value(doc_index):\n",
    "    url = urllib.parse.unquote(webpage_corpus[0][doc_index])\n",
    "    return ranked_webpages[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.054620590299883e-05\n",
      "2.8554413385059092e-05\n"
     ]
    }
   ],
   "source": [
    "print(page_rank_value(123))\n",
    "print(page_rank_value(635))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cmiw.cn/thread-240107-1-1.html\n",
      "http://www.cmiw.cn/thread-967758-1-1.html\n"
     ]
    }
   ],
   "source": [
    "pagerank_results = search_webpage_with_sort(\"深圳机械师傅\", get_page_rank_value)\n",
    "pagerank_url_results = [show_webpage(id_, webpage_corpus[0]) for id_ in pagerank_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以发现PageRank的排名与TF-IDF加权值方法排名不同，可以考虑综合两者权重重新排名\n",
    "- 由于时间关系这里不做继续研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

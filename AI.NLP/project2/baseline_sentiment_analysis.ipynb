{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE='data/trainingset.csv'\n",
    "TEST_DATA_FILE='data/testa.csv'\n",
    "VALIDATE_DATE_FILE = 'data/validationset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_only = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "validation = pd.read_csv(VALIDATE_DATE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train_only, validation],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments = train.columns.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_filter(content):\n",
    "    return content.replace('\\\"','').replace('，','').replace('。','').replace('【',\n",
    "    '').replace('】','').replace('\\n','').replace('；','').replace('.','').replace('～','')\n",
    "\n",
    "import jieba\n",
    "\n",
    "def cut(content):\n",
    "    return ' '.join(jieba.cut(content))\n",
    "\n",
    "    \n",
    "def punc_cut(content):\n",
    "    return punctuation_filter(cut(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_data(train, test, max_features, maxlen):\n",
    "    \"\"\"\n",
    "    Convert data to proper format.\n",
    "    1) Shuffle\n",
    "    2) Lowercase\n",
    "    3) Sentiments to Categorical\n",
    "    4) Tokenize and Fit\n",
    "    5) Convert to sequence (format accepted by the network)\n",
    "    6) Pad\n",
    "    7) Voila!\n",
    "    \"\"\"\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    train = train.sample(frac=1).reset_index(drop=True)\n",
    "    train['content'] = train['content'].apply(lambda x: punc_cut(x))\n",
    "    test['content'] = test['content'].apply(lambda x: punc_cut(x))\n",
    "\n",
    "    X = train['content']\n",
    "    test_X = test['content']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=maxlen)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    return X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Mark\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.259 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "maxlen = 400\n",
    "max_features = 20000\n",
    "\n",
    "X, test_X = format_data(train, test, max_features, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_price_cost_effective.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 387s - loss: 0.7225 - acc: 0.7651 - val_loss: 0.7263 - val_acc: 0.7597\n",
      "Epoch 2/5\n",
      " - 398s - loss: 0.7017 - acc: 0.7653 - val_loss: 0.7409 - val_acc: 0.7597\n",
      "Epoch 3/5\n",
      " - 439s - loss: 0.5470 - acc: 0.7937 - val_loss: 0.8873 - val_acc: 0.7153\n",
      "Epoch 4/5\n",
      " - 389s - loss: 0.2940 - acc: 0.8922 - val_loss: 1.2716 - val_acc: 0.6746\n",
      "Epoch 5/5\n",
      " - 389s - loss: 0.1530 - acc: 0.9457 - val_loss: 1.6767 - val_acc: 0.6630\n",
      "model_price_discount.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 397s - loss: 0.9952 - acc: 0.6142 - val_loss: 1.0014 - val_acc: 0.6073\n",
      "Epoch 2/5\n",
      " - 389s - loss: 0.9783 - acc: 0.6143 - val_loss: 1.0139 - val_acc: 0.6073\n",
      "Epoch 3/5\n",
      " - 399s - loss: 0.8310 - acc: 0.6143 - val_loss: 1.1557 - val_acc: 0.6073\n",
      "Epoch 4/5\n",
      " - 398s - loss: 0.5945 - acc: 0.6143 - val_loss: 1.6645 - val_acc: 0.6073\n",
      "Epoch 5/5\n",
      " - 399s - loss: 0.4696 - acc: 0.6155 - val_loss: 2.1125 - val_acc: 0.6073\n",
      "model_environment_decoration.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 402s - loss: 1.0119 - acc: 0.5133 - val_loss: 1.0058 - val_acc: 0.5169\n",
      "Epoch 2/5\n",
      " - 401s - loss: 0.9936 - acc: 0.5134 - val_loss: 1.0195 - val_acc: 0.5169\n",
      "Epoch 3/5\n",
      " - 390s - loss: 0.8712 - acc: 0.5408 - val_loss: 1.1686 - val_acc: 0.4879\n",
      "Epoch 4/5\n",
      " - 390s - loss: 0.6431 - acc: 0.6955 - val_loss: 1.4672 - val_acc: 0.4453\n",
      "Epoch 5/5\n",
      " - 407s - loss: 0.4234 - acc: 0.8190 - val_loss: 1.8406 - val_acc: 0.4452\n",
      "model_environment_noise.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 405s - loss: 0.8394 - acc: 0.6988 - val_loss: 0.8283 - val_acc: 0.7020\n",
      "Epoch 2/5\n",
      " - 416s - loss: 0.8188 - acc: 0.6990 - val_loss: 0.8442 - val_acc: 0.7020\n",
      "Epoch 3/5\n",
      " - 416s - loss: 0.6728 - acc: 0.6996 - val_loss: 0.9760 - val_acc: 0.6909\n",
      "Epoch 4/5\n",
      " - 399s - loss: 0.4216 - acc: 0.8276 - val_loss: 1.3060 - val_acc: 0.6148\n",
      "Epoch 5/5\n",
      " - 385s - loss: 0.2201 - acc: 0.9176 - val_loss: 1.7674 - val_acc: 0.5972\n",
      "model_environment_space.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 390s - loss: 1.0116 - acc: 0.6230 - val_loss: 1.0050 - val_acc: 0.6266\n",
      "Epoch 2/5\n",
      " - 383s - loss: 0.9924 - acc: 0.6232 - val_loss: 1.0221 - val_acc: 0.6266\n",
      "Epoch 3/5\n",
      " - 386s - loss: 0.8424 - acc: 0.6232 - val_loss: 1.1511 - val_acc: 0.6266\n",
      "Epoch 4/5\n",
      " - 383s - loss: 0.5818 - acc: 0.7153 - val_loss: 1.6057 - val_acc: 0.5554\n",
      "Epoch 5/5\n",
      " - 361s - loss: 0.3565 - acc: 0.8598 - val_loss: 2.0784 - val_acc: 0.5004\n",
      "model_environment_cleaness.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 360s - loss: 0.9255 - acc: 0.6336 - val_loss: 0.9113 - val_acc: 0.6384\n",
      "Epoch 2/5\n",
      " - 406s - loss: 0.9078 - acc: 0.6338 - val_loss: 0.9329 - val_acc: 0.6384\n",
      "Epoch 3/5\n",
      " - 418s - loss: 0.7672 - acc: 0.6592 - val_loss: 1.0661 - val_acc: 0.5822\n",
      "Epoch 4/5\n",
      " - 418s - loss: 0.4965 - acc: 0.7948 - val_loss: 1.3642 - val_acc: 0.5359\n",
      "Epoch 5/5\n",
      " - 422s - loss: 0.2846 - acc: 0.8908 - val_loss: 1.9011 - val_acc: 0.5270\n",
      "model_dish_portion.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 489s - loss: 1.1334 - acc: 0.5413 - val_loss: 1.1358 - val_acc: 0.5432\n",
      "Epoch 2/5\n",
      " - 358s - loss: 1.1183 - acc: 0.5415 - val_loss: 1.1386 - val_acc: 0.5432\n",
      "Epoch 3/5\n",
      " - 362s - loss: 0.9857 - acc: 0.5541 - val_loss: 1.2682 - val_acc: 0.5196\n",
      "Epoch 4/5\n",
      " - 360s - loss: 0.7066 - acc: 0.6935 - val_loss: 1.6025 - val_acc: 0.4668\n",
      "Epoch 5/5\n",
      " - 361s - loss: 0.4455 - acc: 0.8204 - val_loss: 2.0500 - val_acc: 0.4072\n",
      "model_dish_taste.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 362s - loss: 0.9899 - acc: 0.5276 - val_loss: 0.9836 - val_acc: 0.5239\n",
      "Epoch 2/5\n",
      " - 360s - loss: 0.9715 - acc: 0.5277 - val_loss: 0.9941 - val_acc: 0.5239\n",
      "Epoch 3/5\n",
      " - 364s - loss: 0.8480 - acc: 0.5665 - val_loss: 1.1470 - val_acc: 0.4916\n",
      "Epoch 4/5\n",
      " - 380s - loss: 0.6109 - acc: 0.7169 - val_loss: 1.4479 - val_acc: 0.4690\n",
      "Epoch 5/5\n",
      " - 367s - loss: 0.3799 - acc: 0.8416 - val_loss: 1.9216 - val_acc: 0.4474\n",
      "model_dish_look.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 359s - loss: 0.8103 - acc: 0.7225 - val_loss: 0.8016 - val_acc: 0.7229\n",
      "Epoch 2/5\n",
      " - 358s - loss: 0.7888 - acc: 0.7227 - val_loss: 0.8192 - val_acc: 0.7229\n",
      "Epoch 3/5\n",
      " - 361s - loss: 0.6315 - acc: 0.7227 - val_loss: 0.9908 - val_acc: 0.7229\n",
      "Epoch 4/5\n",
      " - 356s - loss: 0.4119 - acc: 0.7228 - val_loss: 1.4990 - val_acc: 0.7229\n",
      "Epoch 5/5\n",
      " - 357s - loss: 0.3144 - acc: 0.7253 - val_loss: 1.7179 - val_acc: 0.7226\n",
      "model_dish_recommendation.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 356s - loss: 0.6230 - acc: 0.8083 - val_loss: 0.6272 - val_acc: 0.8028\n",
      "Epoch 2/5\n",
      " - 355s - loss: 0.5983 - acc: 0.8085 - val_loss: 0.6438 - val_acc: 0.8028\n",
      "Epoch 3/5\n",
      " - 355s - loss: 0.4391 - acc: 0.8378 - val_loss: 0.8009 - val_acc: 0.7796\n",
      "Epoch 4/5\n",
      " - 356s - loss: 0.2086 - acc: 0.9250 - val_loss: 1.2241 - val_acc: 0.7583\n",
      "Epoch 5/5\n",
      " - 358s - loss: 0.1082 - acc: 0.9614 - val_loss: 1.6033 - val_acc: 0.6975\n",
      "model_others_overall_experience.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 360s - loss: 0.9072 - acc: 0.6659 - val_loss: 0.8909 - val_acc: 0.6732\n",
      "Epoch 2/5\n",
      " - 357s - loss: 0.8872 - acc: 0.6661 - val_loss: 0.9070 - val_acc: 0.6732\n",
      "Epoch 3/5\n",
      " - 359s - loss: 0.7313 - acc: 0.6661 - val_loss: 1.0456 - val_acc: 0.6732\n",
      "Epoch 4/5\n",
      " - 358s - loss: 0.4879 - acc: 0.7315 - val_loss: 1.4692 - val_acc: 0.5728\n",
      "Epoch 5/5\n",
      " - 358s - loss: 0.2972 - acc: 0.8853 - val_loss: 1.8527 - val_acc: 0.5516\n",
      "model_others_willing_to_consume_again.h5\n",
      "Train on 90000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      " - 360s - loss: 0.8893 - acc: 0.6245 - val_loss: 0.8836 - val_acc: 0.6241\n",
      "Epoch 2/5\n",
      " - 368s - loss: 0.8677 - acc: 0.6248 - val_loss: 0.9054 - val_acc: 0.6241\n",
      "Epoch 3/5\n",
      " - 412s - loss: 0.7164 - acc: 0.6248 - val_loss: 1.0599 - val_acc: 0.6241\n",
      "Epoch 4/5\n",
      " - 423s - loss: 0.5152 - acc: 0.6249 - val_loss: 1.3634 - val_acc: 0.6241\n",
      "Epoch 5/5\n",
      " - 413s - loss: 0.4205 - acc: 0.6279 - val_loss: 1.7911 - val_acc: 0.6240\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def new_model():\n",
    "    model_ = Sequential()\n",
    "\n",
    "    # Input / Embdedding\n",
    "    model_.add(Embedding(max_features, 150, input_length=maxlen))\n",
    "\n",
    "    # CNN\n",
    "    model_.add(SpatialDropout1D(0.2))\n",
    "\n",
    "    model_.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model_.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model_.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model_.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model_.add(Flatten())\n",
    "\n",
    "    # Output layer\n",
    "    model_.add(Dense(4, activation='sigmoid'))\n",
    "    model_.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_\n",
    "\n",
    "for senti in sentiments:\n",
    "    print('model_'+ senti +'.h5')\n",
    "    Y_i = to_categorical(train[senti].values, 4)\n",
    "    X_train, X_val, Y_train_i, Y_val_i = train_test_split(X, Y_i, test_size=0.25, random_state=seed)\n",
    "    model_ = new_model()\n",
    "    model_.fit(X_train, Y_train_i, validation_data=(X_val, Y_val_i), epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    model_.save('model_'+ senti +'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
